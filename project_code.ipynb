{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1lSLtfh0PX2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "import keras.backend as K\n",
    "\n",
    "from preprocessing import Preprocessing\n",
    "from model import FullyConvolutionalNetwork\n",
    "from metrics import Metrics\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mu2nVi8ut6KC"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CJbRjw0S0Zfz"
   },
   "outputs": [],
   "source": [
    "# !unzip gdrive/My\\ Drive/PASCALVOCdataset12000images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GIQAef6jP0jZ"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 22\n",
    "height, width = 224, 224\n",
    "imagePath = \"PASCALVOCdataset12000images/Images/\"\n",
    "annotationPath = \"PASCALVOCdataset12000images/SegmentationClassAug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-90y4ifHP0kE"
   },
   "outputs": [],
   "source": [
    "len(os.listdir(imagePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTELUejtP0kN"
   },
   "outputs": [],
   "source": [
    "prePro = Preprocessing(height, width, NUM_CLASSES)\n",
    "trainImages, valImages, testImages = prePro.get_test_train_filenames(imagePath, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QR86_5Ls0PYR"
   },
   "outputs": [],
   "source": [
    "len(trainImages), len(valImages), len(testImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZeLmuye0PYZ"
   },
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(3, 2, figsize=(15, 15))\n",
    "for n, d in enumerate(prePro.data_gen(trainImages, imagePath, annotationPath, 1)):\n",
    "    _, h, w, c = d[0].shape\n",
    "    axs[n][0].imshow(d[0].reshape(h, w, c))\n",
    "    axs[n][1].imshow(np.argmax(d[1], axis=3).reshape(h, w))\n",
    "    if(n == 2):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aLrBjLla0PYf"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import math_ops, array_ops\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "def new_sparse_categorical_accuracy(y_true, y_pred):\n",
    "    y_pred_rank = ops.convert_to_tensor(y_pred).get_shape().ndims\n",
    "    y_true_rank = ops.convert_to_tensor(y_true).get_shape().ndims\n",
    "\n",
    "    # If the shape of y_true is (num_samples, 1), squeeze to (num_samples,)\n",
    "    if (y_true_rank is not None) and (y_pred_rank is not None) and (len(K.int_shape(y_true)) == len(K.int_shape(y_pred))):\n",
    "        y_true = array_ops.squeeze(y_true, [-1])\n",
    "    y_pred = math_ops.argmax(y_pred, axis=-1)\n",
    "    # If the predicted output and actual output types don't match, force cast them\n",
    "    # to match.\n",
    "    if K.dtype(y_pred) != K.dtype(y_true):\n",
    "        y_pred = math_ops.cast(y_pred, K.dtype(y_true))\n",
    "    return math_ops.cast(math_ops.equal(y_true, y_pred), K.floatx())\n",
    "\n",
    "#credits : https://github.com/keras-team/keras/issues/11348#issuecomment-468568429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oo1s1C8m_A93"
   },
   "source": [
    "# Fully Convolution Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OcRNjjBjSxbo"
   },
   "outputs": [],
   "source": [
    "from model import FullyConvolutionalNetwork\n",
    "model = FullyConvolutionalNetwork((height, width, 3), NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SuRbxfk5S66m"
   },
   "source": [
    "## Traditional FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O78nrMFhSq5E"
   },
   "outputs": [],
   "source": [
    "fcn = model.get_model()\n",
    "fcn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qdszIEdATLuT"
   },
   "outputs": [],
   "source": [
    "fcn.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GXa1JrAkTyjB"
   },
   "outputs": [],
   "source": [
    "history_fcn = fcn.fit_generator(prePro.data_gen(trainImages, imagePath, annotationPath, 32), steps_per_epoch=128, epochs = 1, \n",
    "                            validation_data=prePro.data_gen(valImages, imagePath, annotationPath, 32), validation_steps=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGRhrknUahPn"
   },
   "outputs": [],
   "source": [
    "fcn.save(\"gdrive/My Drive/Traditional_FCN(Adam).h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fOep1nuHSoJJ"
   },
   "source": [
    "## Modified FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g-aUgbra0PYr"
   },
   "outputs": [],
   "source": [
    "modified_fcn = model.get_modified_model()\n",
    "modified_fcn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x3Ia2CnKVf21"
   },
   "outputs": [],
   "source": [
    "modified_fcn.compile(optimizer=Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNIh3AeiVgAw"
   },
   "outputs": [],
   "source": [
    "history_modified_fcn = modified_fcn.fit_generator(prePro.data_gen(trainImages, imagePath, annotationPath, 32), steps_per_epoch=128, epochs = 1, \n",
    "                            validation_data=prePro.data_gen(valImages, imagePath, annotationPath, 32), validation_steps=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FmbyEvWyanlT"
   },
   "outputs": [],
   "source": [
    "modified_fcn.save(\"gdrive/My Drive/Modified_FCN(Adam).h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PhIzuVnR0PY6"
   },
   "outputs": [],
   "source": [
    "# import IPython\n",
    "# import keras\n",
    "# keras.utils.plot_model(fcn, to_file='fcn_modified_model.png', show_shapes=True)\n",
    "# IPython.display.Image('fcn_modified_model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vVG4oIUBlEdx"
   },
   "source": [
    "# Results and Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "roaJjWNrEz5m"
   },
   "source": [
    "## Traditional Fully Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zt7nGHNAJB3V"
   },
   "outputs": [],
   "source": [
    "met_fcn = Metrics(imagePath, annotationPath, fcn, prePro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEDWMbaKFATe"
   },
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQEsCVJwgpGz"
   },
   "outputs": [],
   "source": [
    "met_fcn.plot_predictions(valImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2j_aGh9L0PZR"
   },
   "outputs": [],
   "source": [
    "met_fcn.plot_graphs('acc', 'epoch', 'accuracy', 'Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZI4oAbm0PZW"
   },
   "outputs": [],
   "source": [
    "met_fcn.plot_graphs('loss', 'epoch', 'loss', 'Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KO525tqqoOt6"
   },
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TMXY6EJ0rSI7"
   },
   "outputs": [],
   "source": [
    "met_fcn.plot_predictions(testImages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6Y7yk8Mox9c"
   },
   "source": [
    "## Modified Fully Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BJrPAN1DosEA"
   },
   "outputs": [],
   "source": [
    "met_modified_fcn = Metrics(imagePath, annotationPath, modified_fcn, prePro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9lJWZUDevk9X"
   },
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5EdnjmpgpOiD"
   },
   "outputs": [],
   "source": [
    "met_modified_fcn.plot_predictions(valImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6DIp6eZdvqow"
   },
   "outputs": [],
   "source": [
    "met_modified_fcn.plot_graphs('acc', 'epoch', 'accuracy', 'Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0QU9GCiv15t"
   },
   "outputs": [],
   "source": [
    "met_modified_fcn.plot_graphs('loss', 'epoch', 'loss', 'Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RGHxC9wgvrMA"
   },
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e5vQ2jzivs2n"
   },
   "outputs": [],
   "source": [
    "met_modified_fcn.plot_predictions(testImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mOMLGIP6v_Gb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "project_code.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
